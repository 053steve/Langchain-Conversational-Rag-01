{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import speech_recognition as sr\n",
    "\n",
    "# This uses Google Speech Recognition \n",
    "\n",
    "# Path to the audio file\n",
    "audio_file_path = \"assets/what_is_task_decomposition.wav\"\n",
    "audio_file_path_2 = \"assets/what_is_this_article_about.wav\"\n",
    "base_language = 'en'\n",
    "\n",
    "\n",
    "def transcribe_audio(audio_file_path: str, language: str = 'en') -> str:\n",
    "    # Create a recognizer object\n",
    "    r = sr.Recognizer()\n",
    "    # Use the audio file as the audio source\n",
    "    with sr.AudioFile(audio_file_path) as source:\n",
    "        # Record the audio from the file\n",
    "        audio = r.record(source)\n",
    "        \n",
    "        try:\n",
    "            # Recognize speech using Google Speech Recognition + set it to listen to Thai\n",
    "            # [NOTES] It cannot listen to both thai and english in the same file\n",
    "            # text_query = r.recognize_google(audio, language=\"th\")\n",
    "            text_query = r.recognize_google(audio, language=language)\n",
    "            return text_query            \n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Sorry, I couldn't understand what you said.\")\n",
    "        except sr.RequestError as e:\n",
    "            print(\"Sorry, an error occurred. Please check your internet connection.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "first_query = transcribe_audio(audio_file_path, base_language)\n",
    "second_query = transcribe_audio(audio_file_path_2, base_language)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task decomposition is a technique used to break down a complex task into smaller and more manageable subtasks or steps. This process helps in structuring the problem-solving approach and allows for better planning and organization of the overall task. Techniques like Chain of Thought (CoT) guide models to think step by step, transforming large tasks into simpler components for improved performance and understanding of the thinking process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_core.tracers.core:Parent run 0bf03a61-7abe-4ee5-a89e-faeedfe61d90 not found for run fb77947c-a5d7-4edf-95ff-8a3b741774ec. Treating as a root run.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The article discusses the challenges and limitations associated with building LLM-centered agents. It highlights common issues that arise when developing agents based on large language models.\n",
      "User: what is Task decomposition\n",
      "\n",
      "AI: Task decomposition involves breaking down a complex task into smaller, more manageable steps or subtasks. It allows for a more systematic approach to problem-solving by dividing the overall task into simpler components. Task decomposition can be done using techniques like Chain of Thought (CoT), which prompts models to think step by step and decompose difficult tasks into easier parts for better understanding and execution.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# send text to openai and use the text to generate a response \n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.chains import create_retrieval_chain, create_history_aware_retriever\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "import bs4\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "\n",
    "# 1. Load, chunk and index the contents of the blog to create a retriever.\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")\n",
    "\n",
    "\n",
    "# 2. Incorporate the retriever into a question-answering chain.\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "question = first_query\n",
    "ai_msg_1 = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content=question),\n",
    "        AIMessage(content=ai_msg_1[\"answer\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(ai_msg_1[\"answer\"])\n",
    "\n",
    "second_question = second_query\n",
    "ai_msg_2 = rag_chain.invoke({\"input\": second_question, \"chat_history\": chat_history})\n",
    "\n",
    "\n",
    "print(ai_msg_2[\"answer\"])\n",
    "\n",
    "### Statefully manage chat history ###\n",
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")\n",
    "\n",
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": question},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"abc123\"}\n",
    "    },  # constructs a key \"abc123\" in `store`.\n",
    ")[\"answer\"]\n",
    "\n",
    "for message in store[\"abc123\"].messages:\n",
    "    if isinstance(message, AIMessage):\n",
    "        prefix = \"AI\"\n",
    "    else:\n",
    "        prefix = \"User\"\n",
    "\n",
    "    print(f\"{prefix}: {message.content}\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sh: start: command not found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32512"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform text to audio and play it back\n",
    "\n",
    "from gtts import gTTS\n",
    "import os\n",
    "\n",
    "\n",
    "gtts_obj = gTTS(text=ai_msg_2[\"answer\"], lang=base_language, slow=False)\n",
    "\n",
    "# Saving the converted audio in a mp3 file named\n",
    "# welcome \n",
    "gtts_obj.save(\"answer.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform text to audio and play it back\n",
    "\n",
    "import pygame\n",
    "\n",
    "# Initialize the mixer module\n",
    "pygame.mixer.init()\n",
    "\n",
    "# Load the mp3 file\n",
    "pygame.mixer.music.load(\"answer.mp3\")\n",
    "\n",
    "# Play the loaded mp3 file\n",
    "pygame.mixer.music.play()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
